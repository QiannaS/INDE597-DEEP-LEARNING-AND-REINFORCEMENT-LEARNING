{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mountain car.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'This code might take a long time to run'\n",
        "'Another sample code from https://github.com/kumarnikhil936/q_learning_mountain_car_openai might be easier to run'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import gym\n",
        "import csv\n",
        "import os\n",
        "import pickle\n",
        "from queue import Queue\n",
        "import pickle\n",
        "\n",
        "class QLearning:\n",
        "    def __init__(self, actions_space, learning_rate=0.01, reward_decay=0.99, e_greedy=0.6):\n",
        "        self.actions = actions_space    \n",
        "        #self.target                    \n",
        "        self.lr = learning_rate         \n",
        "        self.gamma = reward_decay       \n",
        "        self.epsilon = e_greedy        \n",
        "        self.num_pos = 20               \n",
        "        self.num_vel = 14               \n",
        "        \n",
        "        self.q_table =  np.random.uniform(low=-1, high=1, size=(self.num_pos*self.num_vel, self.actions.n)) \n",
        "        self.pos_bins = self.toBins(-1.2, 0.6, self.num_pos)\n",
        "        self.vel_bins = self.toBins(-0.07, 0.07, self.num_vel)\n",
        "\n",
        "    \n",
        "    def choose_action(self,state):\n",
        "       \n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            \n",
        "            action = np.argmax(self.q_table[state])\n",
        "        else:\n",
        "            \n",
        "            action = self.actions.sample()\n",
        "        return action\n",
        "\n",
        "    \n",
        "    def toBins(self,clip_min, clip_max, num):\n",
        "        return np.linspace(clip_min, clip_max, num + 1)\n",
        "   \n",
        "   \n",
        "    def digit(self,x, bin):\n",
        "        n = np.digitize(x,bins = bin)\n",
        "        if x== bin[-1]:\n",
        "            n=n-1\n",
        "        return n\n",
        "\n",
        "    \n",
        "    def digitize_state(self,observation):\n",
        "        \n",
        "        cart_pos, cart_v = observation\n",
        "       \n",
        "        digitized = [self.digit(cart_pos,self.pos_bins),\n",
        "                    self.digit(cart_v,self.vel_bins),]\n",
        "        \n",
        "        return (digitized[1]-1)*self.num_pos + digitized[0]-1\n",
        "\n",
        "    \n",
        "    def learn(self, state, action, r, next_state):\n",
        "        next_action = np.argmax(self.q_table[next_state]) \n",
        "        q_predict = self.q_table[state, action]\n",
        "        q_target = r + self.gamma * self.q_table[next_state, next_action]   \n",
        "        self.q_table[state, action] += self.lr * (q_target - q_predict)     \n",
        "\n",
        "\n",
        "def train():\n",
        "    env = gym.make('MountainCar-v0')   \n",
        "    print(env.action_space)\n",
        "    agent = QLearning(env.action_space)\n",
        "    # with open(os.getcwd()+'/tmp/carmountain.model', 'rb') as f:\n",
        "    #     agent = pickle.load(f)\n",
        "    # agent.actions = env.action_space    \n",
        "    \n",
        "    for i in range(10000):  \n",
        "        observation = env.reset()  \n",
        "        state = agent.digitize_state(observation)  \n",
        "        for t in range(300):   \n",
        "            action = agent.choose_action(state)  \n",
        "            observation, reward, done, info = env.step(action)   \n",
        "            next_state = agent.digitize_state(observation)\n",
        "            # if done:\n",
        "            #     reward-=200  \n",
        "            if reward == 0:  \n",
        "                reward+=1000   \n",
        "            \n",
        "            print(action,reward,done,state,next_state)\n",
        "            agent.learn(state,action,reward,next_state)\n",
        "            state = next_state\n",
        "            if done:    \n",
        "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "                break\n",
        "            # env.render()    \n",
        "    print(agent.q_table)\n",
        "    env.close()\n",
        "    #保存 \n",
        "    with open(os.getcwd()+'/tmp/carmountain.model', 'wb') as f:\n",
        "        pickle.dump(agent, f)\n",
        "\n",
        "def test():\n",
        "    env = gym.make('MountainCar-v0')   \n",
        "    print(env.action_space)\n",
        "    with open(os.getcwd()+'/tmp/carmountain.model', 'rb') as f:\n",
        "        agent = pickle.load(f)\n",
        "    agent.actions = env.action_space    \n",
        "    agent.epsilon = 1\n",
        "    observation = env.reset()  \n",
        "    state = agent.digitize_state(observation)  \n",
        "    \n",
        "    for t in range(500):   \n",
        "        action = agent.choose_action(state)  \n",
        "        observation, reward, done, info = env.step(action)   \n",
        "        next_state = agent.digitize_state(observation)\n",
        "        print(action,reward,done,state,next_state)\n",
        "        agent.learn(state,action,reward,next_state)\n",
        "        state = next_state\n",
        "        env.render()    \n",
        "    env.close()\n",
        "\n",
        "def run_test():\n",
        "    env = gym.make('MountainCar-v0')   \n",
        "    \n",
        "    observation = env.reset()  \n",
        "    \n",
        "    for t in range(500):   \n",
        "        action =  np.random.choice([0, 1, 2]) \n",
        "        #action = 2\n",
        "        observation, reward, done, info = env.step(action)   \n",
        "        print(action,reward,done)\n",
        "        print(observation)\n",
        "        env.render() \n",
        "        time.sleep(0.02)\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train() \n",
        "    test()    \n",
        "\n",
        "   # run_test()"
      ],
      "metadata": {
        "id": "eSucz9D03OL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}